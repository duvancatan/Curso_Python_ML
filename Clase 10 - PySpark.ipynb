{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://files.realpython.com/media/PySpark-Tutorial_Watermarked.305db668c8cb.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué es Apache Spark?\n",
    "Spark es una solución de big data que ha demostrado ser más fácil y rápida que Hadoop MapReduce. Spark es un software de código abierto desarrollado por UC Berkeley RAD lab en 2009. Desde que fue lanzado al público en 2010, Spark ha crecido en popularidad y se utiliza a través de la industria con una escala sin precedentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué es Pyspark?\n",
    "\n",
    "Spark es el nombre del motor para realizar la computación en clúster, mientras que PySpark es la biblioteca de Python para usar Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo funciona Spark?\n",
    "Spark se basa en el motor computacional, lo que significa que se encarga de la programación, distribución y supervisión de la aplicación. Cada tarea se realiza en varios equipos de trabajo llamados clúster de computación. Un clúster informático hace referencia a la división de tareas. Una máquina realiza una tarea, mientras que las otras contribuyen a la salida final a través de una tarea diferente. Al final, todas las tareas se agregan para producir una salida. El administrador de Spark ofrece una visión general 360 de varios trabajos de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://guru99.es/wp-content/uploads/2020/02/082918_1213_ApacheSpark1-1500x566.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark está diseñado para funcionar con\n",
    "* Python\n",
    "* Java\n",
    "* Scala\n",
    "* SQL\n",
    "Una característica significativa de Spark es la gran cantidad de biblioteca incorporada, incluyendo MLLib para el aprendizaje automático. Spark también está diseñado para trabajar con clústeres de Hadoop y puede leer el amplio tipo de archivos, incluyendo datos Hive, CSV, JSON, datos de Casandra entre otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter\n",
    "Abra Jupyter Notebook y pruebe si PySpark funciona. En un nuevo bloc de notas pegue el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-49d7c4e178f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifactory.apps.bancolombia.com/api/pypi/pypi-bancolombia/simple\n",
      "Collecting pyspark\n",
      "  Downloading https://artifactory.apps.bancolombia.com/api/pypi/pypi-bancolombia/packages/packages/8e/b0/bf9020b56492281b9c9d8aae8f44ff51e1bc91b3ef5a884385cb4e389a40/pyspark-3.0.0.tar.gz (204.7MB)\n",
      "Collecting py4j==0.10.9 (from pyspark)\n",
      "  Downloading https://artifactory.apps.bancolombia.com/api/pypi/pypi-bancolombia/packages/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Running setup.py bdist_wheel for pyspark: started\n",
      "  Running setup.py bdist_wheel for pyspark: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\ducatano\\AppData\\Local\\pip\\Cache\\wheels\\d0\\dc\\03\\4ed1f281599356f993ce850cfbce5fcc232d688eb26a691d0b\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark -i https://artifactory.apps.bancolombia.com/api/pypi/pypi-bancolombia/simple --trusted-host artifactory.apps.bancolombia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Spark es un **\"framework\" (entorno de trabajo) gratuito y de código abierto para la computación en \"clusters\" de ordenadores**.</br>\n",
    "\n",
    "<br>**¿Pero qué es un \"cluster\"?** </br>\n",
    "\n",
    "<br>Un \"cluster\" es una agrupación o conglomerado de ordenadores con un mismo \"hardware\" común que se comportan como si fuesen un único ordenador, que se usan mayoritariamente para procesar cálculos complejos o grandes volúmenes de datos</br>\n",
    "\n",
    "<img src=\"https://sites.google.com/site/sdistribuidoscluster/_/rsrc/1472854955027/proceso/actividad-1/CLUSTER.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las principales ventajas de Spark es que **combina esta capacidad de computación distribuida con un código sencillo y elegante, que además ofrece soporte a múltiples lenguajes de programación**, entre ellos el Python. de la fusión entre Python y Spark nace el paquete **PySpark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Posee dos funcionalidades:\n",
    "1. Los grafos acíclicos dirigidos, del inglés \"directed acyclic graph\"\n",
    "2. Bases de datos distribuidas y resilientes, del inglés \"resilient distributed dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafos acíclicos dirigidos\n",
    "\n",
    "Desde una etapa de trabajo o estado concreto de nuestro proyecto vamos a realizar múltiples procesos, transformaciones de nuestros datos, acciones, etc., algunos de los cuales van a realizarse de forma paralela, pero a lo largo de todos los procesos nunca vamos a regresar al estado en el que nos encontrábamos al principio. \n",
    "\n",
    "1. \"Dirigido\",  significa que cada borde tiene una dirección definida. Por lo que cada borde necesariamente representa un flujo de datos direccional único de un vértice a otro. \n",
    "2. \"Acíclico\" significa que no hay bucles o ciclos en el gráfico. Esto significa que, para cualquier vértice dado, si sigue un borde que conecta ese vértice con otro, no hay camino en el gráfico para volver a ese vértice inicial.\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/Dd7VYnExW97KixnE_R_-dnD5InTAYDK2wKv8pAtcJoyV9lxbHJxvvGxRwkMptga0t5z8TNK7RcBF0YlPPKlVyqMoJHcggFtd-7eFVNiRPyAl9G9Whu2XNBzBBE5ZPourNTJxrVl4\"/>\n",
    "\n",
    "Un Directed Acyclic Graph o Grafo Acíclico Dirigido (DAG), **es un tipo de grafo por el cual se puede representar una serie de datos relacionados entre sí**. Estos datos se presentan visualmente como un conjunto de círculos o nodos. Cada uno de estos **círculos o nodos, representa un determinado conjunto de datos dentro de todo el grupo**. A la vez, estos nodos están conectados por líneas, que representan el **flujo de datos de un punto a otro dentro del grafo**.Cada círculo se conoce como un \"vértice\" y cada línea se conoce como un \"borde\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient distributed dataset (RDD)\n",
    "Estructura de **almacenaje distribuida en particiones**, lo que permite a los programadores realizar operaciones sobre grandes cantidades de datos en \"clusters\" de manera rápida y a prueba de fallos. Esta nueva estructura de datos se originó en respuesta a los problemas que se generaban al manejar datos de manera ineficiente, especialmente a la hora de ejecutar algoritmos iterativos y procesos de minería de datos.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/583/1*rtyvHjINsHF_5yHf8c0Z0Q.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un programa típico de Spark se estructura de la siguiente forma: \n",
    "\n",
    "1. Crea un \"resilient distributed dataset\" RDD . Todos y cada uno de los conjuntos de datos en Spark RDD se dividen lógicamente en muchos nodos para que se puedan calcular en diferentes nodos del clúster. \n",
    "\n",
    "<img src=\"https://intellipaat.com/mediaFiles/2015/08/Resilient-Distributed-Datasets-RDDs.jpg\"/>\n",
    "\n",
    "2. Transformaciones deseadas para crear más objetos RDD a partir del primero. Estas transformaciones no eliminan el RDD original, sino que crean nuevos. \n",
    "\n",
    "3. Realizar acciones sobre estos RDD y posiblemente más transformaciones sobre los datos. \n",
    "\n",
    "4. Guardar la RDD final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy posible que los datos con los que se necesite trabajar estén en diferentes objetos RDD, por lo que Spark define dos tipos de opciones de transformación\n",
    "\n",
    "<img src=\"t.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "Las ventas de los años 2017 y 2016, en dos objetos distintos. Pero por otro lado solo queremos ver las ventas de productos de estos dos años que superen los, digamos, 1000 euros, para solo trabajar con los productos más exitosos.\n",
    "<img src=\"e.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL \n",
    "\n",
    "Es un módulo de Apache Spark para el procesamiento de datos estructurados. ... Conceptualmente es equivalente a una tabla en una base de datos relacional. Los DataFrames en Spark tienen las mismas capacidades que los RDDs, como por ejemplo su inmutabilidad, en memoria, resilientes, computación distribuida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark Streaming\n",
    "\n",
    "Spark Streaming es una extensión de la API central de Spark que permite a los ingenieros de datos y científicos de datos procesar datos en tiempo real de varias fuentes, incluidas (entre otras) Kafka, Flume y Amazon Kinesis. Estos datos procesados ​​se pueden enviar a sistemas de archivos, bases de datos y paneles en vivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML\n",
    "\n",
    "Tiene como objetivo proporcionar un conjunto uniforme de API de alto nivel que ayudan a los usuarios a crear y ajustar  prácticas de aprendizaje automático. Spark MLlib aporta algoritmos tanto de aprendizaje supervisado como no supervisado que ofrece soluciones para el mundo del Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Vamos a usarlo con esta función y vamos a tener que especificar un 'Master', \n",
    "aquí vamos a poner 'local', ya que como no estamos gestionando nada desde un \"cluster\" \n",
    "estamos actuando en local, y vamos a darle un nombre, puede ser cualquiera –'Mi programa\n",
    "'''\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf= SparkConf().setMaster(\"local\").setAppName(\"Mi programa\")\n",
    "sc =SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> **Mira si vuelves a correr de nuevo el contexto ¿ Qué sucede?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El link podemos seguir que está pasando con mi código\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=sc.textFile(\"Names.txt\")\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿ Cuántas filas tiene?\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primera linea\n",
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevo RDD. Un remuestro del 10% de lo casos que tengo del original, sin reemplazo\n",
    "lines2 = lines.sample(fraction=0.1, withReplacement= False)\n",
    "#Aquí no ha ejecutado nada, está pendiente de que ejecutemos una acción para ejecutarse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nuevo inicio según el remuestreo\n",
    "lines2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes en PySpark\n",
    "<img src=\"pandas.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>credit</th>\n",
       "      <th>prestamo</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>r</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>92.893</td>\n",
       "      <td>-46.2</td>\n",
       "      <td>1.313</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.855</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jun</td>\n",
       "      <td>wed</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.465</td>\n",
       "      <td>-41.8</td>\n",
       "      <td>4.962</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jun</td>\n",
       "      <td>fri</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.465</td>\n",
       "      <td>-41.8</td>\n",
       "      <td>4.959</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>93.200</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>4.191</td>\n",
       "      <td>5195.8</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          job  marital          education default   credit prestamo  \\\n",
       "0   30  blue-collar  married           basic.9y      no      yes       no   \n",
       "1   39     services   single        high.school      no       no       no   \n",
       "2   25     services  married        high.school      no      yes       no   \n",
       "3   38     services  married           basic.9y      no  unknown  unknown   \n",
       "4   47       admin.  married  university.degree      no      yes       no   \n",
       "\n",
       "     contact month day_of_week ...  campaign  pdays  previous     poutcome  \\\n",
       "0   cellular   may         fri ...         2    999         0  nonexistent   \n",
       "1  telephone   may         fri ...         4    999         0  nonexistent   \n",
       "2  telephone   jun         wed ...         1    999         0  nonexistent   \n",
       "3  telephone   jun         fri ...         3    999         0  nonexistent   \n",
       "4   cellular   nov         mon ...         1    999         0  nonexistent   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx      r  nr.employed   y  \n",
       "0         -1.8          92.893          -46.2  1.313       5099.1  no  \n",
       "1          1.1          93.994          -36.4  4.855       5191.0  no  \n",
       "2          1.4          94.465          -41.8  4.962       5228.1  no  \n",
       "3          1.4          94.465          -41.8  4.959       5228.1  no  \n",
       "4         -0.1          93.200          -42.0  4.191       5195.8  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Promos.csv\", sep=\";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-49223eae7693>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Necesitamos un contexto para estas data contexto SQL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#option(\"header\",\"true\") inferSchema me reconoce que tipo de variables son\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Necesitamos un contexto para estas data contexto SQL\n",
    "#option(\"header\",\"true\") inferSchema me reconoce que tipo de variables son\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "sqlContext=SQLContext(sc)\n",
    "dfspark = sqlContext.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"Promos.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+-----------+-------+------+--------+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+-----+-----------+---+\n",
      "|age|        job|marital|  education|default|credit|prestamo|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|    r|nr.employed|  y|\n",
      "+---+-----------+-------+-----------+-------+------+--------+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+-----+-----------+---+\n",
      "| 30|blue-collar|married|   basic.9y|     no|   yes|      no| cellular|  may|        fri|     487|       2|  999|       0|nonexistent|        -1.8|        92.893|        -46.2|1.313|     5099.1| no|\n",
      "| 39|   services| single|high.school|     no|    no|      no|telephone|  may|        fri|     346|       4|  999|       0|nonexistent|         1.1|        93.994|        -36.4|4.855|     5191.0| no|\n",
      "+---+-----------+-------+-----------+-------+------+--------+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+-----+-----------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=30, job='blue-collar', marital='married', education='basic.9y', default='no', credit='yes', prestamo='no', contact='cellular', month='may', day_of_week='fri', duration=487, campaign=2, pdays=999, previous=0, poutcome='nonexistent', emp.var.rate=-1.8, cons.price.idx=92.893, cons.conf.idx=-46.2, r=1.313, nr.employed=5099.1, y='no'),\n",
       " Row(age=39, job='services', marital='single', education='high.school', default='no', credit='no', prestamo='no', contact='telephone', month='may', day_of_week='fri', duration=346, campaign=4, pdays=999, previous=0, poutcome='nonexistent', emp.var.rate=1.1, cons.price.idx=93.994, cons.conf.idx=-36.4, r=4.855, nr.employed=5191.0, y='no')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4119"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya en Spark, el método o función para obtener un vistazo del nombre de las variables se denomina .printSchema(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- credit: string (nullable = true)\n",
      " |-- prestamo: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- emp.var.rate: double (nullable = true)\n",
      " |-- cons.price.idx: double (nullable = true)\n",
      " |-- cons.conf.idx: double (nullable = true)\n",
      " |-- r: double (nullable = true)\n",
      " |-- nr.employed: double (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- credit: string (nullable = true)\n",
      " |-- prestamo: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- emp.var.rate: double (nullable = true)\n",
      " |-- cons.price.idx: double (nullable = true)\n",
      " |-- cons.conf.idx: double (nullable = true)\n",
      " |-- r: double (nullable = true)\n",
      " |-- nr.employed: double (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1= dfspark.withColumn(\"previous\",dfspark[\"previous\"].cast(\"integer\"))\n",
    "df1.printSchema()\n",
    "#df1= dfspark.withColumn(\"previous\",dfspark[\"previous\"].cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método **.select** permite la selección de columnas del data frame de spark, por ejemplo si se desea selecciónar solo las variables age y tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|previous|\n",
      "+--------+\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       2|\n",
      "|       0|\n",
      "|       0|\n",
      "|       1|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Visualizar las variables\n",
    "dfspark.select(\"previous\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "\n",
    "dfspark.filter((dfspark[\"y\"] == \"\") | dfspark[\"y\"].isNull() | isnan(dfspark[\"y\"])).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cortar los valores nulos\n",
    "df2= dfspark.na.drop(subset=[\"y\",\"marital\",\"education\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otra forma que no existan columnas con valores nulos desde pyspark\n",
    "df2= df2.filter(\"y is not NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  y|\n",
      "+---+\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "|yes|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"y\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  y|\n",
      "+---+\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "| no|\n",
      "|yes|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.select(\"y\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método .describe permite obtener un resumen rápido de las variables a análizar, en este caso para las variables numericas muestra los estadísticos: conteo, media, desv. estandar, míni y máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+--------+---------+-------+------+--------+---------+-----+-----------+------------------+-----------------+------------------+-------------------+--------+-------------------+-----------------+-------------------+------------------+-----------------+----+\n",
      "|summary|               age|    job| marital|education|default|credit|prestamo|  contact|month|day_of_week|          duration|         campaign|             pdays|           previous|poutcome|       emp.var.rate|   cons.price.idx|      cons.conf.idx|                 r|      nr.employed|   y|\n",
      "+-------+------------------+-------+--------+---------+-------+------+--------+---------+-----+-----------+------------------+-----------------+------------------+-------------------+--------+-------------------+-----------------+-------------------+------------------+-----------------+----+\n",
      "|  count|              4119|   4119|    4119|     4119|   4119|  4119|    4119|     4119| 4119|       4119|              4119|             4119|              4119|               4119|    4119|               4119|             4119|               4119|              4119|             4119|4119|\n",
      "|   mean| 40.11361981063365|   null|    null|     null|   null|  null|    null|     null| null|       null| 256.7880553532411|2.537266326778344| 960.4221898519058|0.19033746054867687|    null|0.08497208060208582|93.57970429716252|-40.499101723719384| 3.621355668851656|5166.481694586143|null|\n",
      "| stddev|10.313361547199827|   null|    null|     null|   null|  null|    null|     null| null|       null|254.70373612073644|2.568159237578134|191.92278580077652|  0.541788323429031|    null| 1.5631144559116772|0.579348804988967|  4.594577506837539|1.7335912227013535|73.66790355721237|null|\n",
      "|    min|                18| admin.|divorced| basic.4y|     no|    no|      no| cellular|  apr|        fri|                 0|                1|                 0|                  0| failure|               -3.4|           92.201|              -50.8|             0.635|           4963.6|  no|\n",
      "|    max|                88|unknown| unknown|  unknown|    yes|   yes|     yes|telephone|  sep|        wed|              3643|               35|               999|                  6| success|                1.4|           94.767|              -26.9|             5.045|           5228.1| yes|\n",
      "+-------+------------------+-------+--------+---------+-------+------+--------+---------+-----+-----------+------------------+-----------------+------------------+-------------------+--------+-------------------+-----------------+-------------------+------------------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.filter((dfspark['marital'] == \"\") | dfspark['marital'].isNull() | isnan(dfspark['marital'])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los métodos que ya tiene habilitado Spark para sus data frame, se pueden emular ciertas funcionalidades del SQL, como agrupar, filtar, selecionar, sin embargo, en algunas ocaciones puede tornarse confuso o en queries de una complejidad mayor se pueden quedar cortos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "| marital|          education|\n",
      "+--------+-------------------+\n",
      "|divorced|        high.school|\n",
      "|divorced|            unknown|\n",
      "| unknown|  university.degree|\n",
      "| married|professional.course|\n",
      "|  single|           basic.9y|\n",
      "| married|           basic.4y|\n",
      "| married|  university.degree|\n",
      "|divorced|           basic.4y|\n",
      "|divorced|           basic.9y|\n",
      "|  single|professional.course|\n",
      "| married|           basic.9y|\n",
      "|divorced|           basic.6y|\n",
      "|  single|           basic.6y|\n",
      "|  single|            unknown|\n",
      "| married|        high.school|\n",
      "| married|            unknown|\n",
      "| unknown|        high.school|\n",
      "|  single|  university.degree|\n",
      "| married|           basic.6y|\n",
      "|divorced|         illiterate|\n",
      "+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleccionando los valores que son distintos en rad y ptratio.\n",
    "dfspark.select('marital','education').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+-----------+-------+-------+--------+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+-----+-----------+---+\n",
      "|age|        job|marital|  education|default| credit|prestamo|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|    r|nr.employed|  y|\n",
      "+---+-----------+-------+-----------+-------+-------+--------+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+-----+-----------+---+\n",
      "| 30|blue-collar|married|   basic.9y|     no|    yes|      no| cellular|  may|        fri|     487|       2|  999|       0|nonexistent|        -1.8|        92.893|        -46.2|1.313|     5099.1| no|\n",
      "| 25|   services|married|high.school|     no|    yes|      no|telephone|  jun|        wed|     227|       1|  999|       0|nonexistent|         1.4|        94.465|        -41.8|4.962|     5228.1| no|\n",
      "| 38|   services|married|   basic.9y|     no|unknown| unknown|telephone|  jun|        fri|      17|       3|  999|       0|nonexistent|         1.4|        94.465|        -41.8|4.959|     5228.1| no|\n",
      "+---+-----------+-------+-----------+-------+-------+--------+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+-----+-----------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Filtrano la data por algún criterio\n",
    "dfspark.filter((dfspark.age > 20) & (dfspark.marital == 'married')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.7880553532411"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# También puedo utilizar otras liberías externas\n",
    "import numpy as np\n",
    "media= np.mean(df2.select(\"duration\").collect())\n",
    "media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mostrar el número de particiones RDD donde esta base de datos está distribuida. Esto depende de como tengamos configurado nuestro ordenador van a ser muchas, pocas, puede ser que solo sea una, puede ser que esté distribuida en 200, dependiendo del tamaño del objeto y de nuestra configuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¿ Cuántas particiones tengo?\n",
    "df2.rdd.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
